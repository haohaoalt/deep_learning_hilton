# 深度学习大作业之跟着Hilton学深度神经网络
## 1. 论文理解
起源于2006年science上的Reducing the dimensionality of data with neural networks 是Hinton影响广泛的代表作，这篇文章也标志着deep learning进入炙热时代。

多层感知机上世纪被提出却没有广泛应用其原因在于对多层非线性网络进行权值优化时很难得到全局的参数。因为一般使用数值优化算法（比如BP算法）时需要随机给网络赋一个值，而当这个权值太大的话，就很容易收敛到“差”的局部收敛点，权值太小的话则在进行误差反向传递时离输入层越近的权值更新越慢。而Hinton设计出来的autoencoder深度网络确能够较快的找到比较好的全局最优点，它是用无监督的方法（这里是RBM）先分开对每层网络进行训练，然后将它当作是初始值来微调，实现数据的降维。这种方法被认为是对PCA的一个非线性泛化方法。

每一层网格的预训练都采用RBM方法，给定一张输入图像，我们可以通过调整网络的权值和偏置值使得网络对该输入图像的能量最低。采用多层网络，即把第一层网络的输出作为第二层网络的输入。并且每增加一个网络层，就会提高网络对输入数据重构的log下界概率值，且上层的网络能够提取出其下层网络更高阶的特征。当网络的预训练过程完成后，我们需要把解码部分重新拿回来展开构成整个网络，然后用真实的数据作为样本标签来微调网络的参数。当网络的输入数据是连续值时，只需将可视层的二进制值改为服从方差为1的高斯分布即可，而第一个隐含层的输出仍然为二进制变量。在实验的分层训练过程中，其第一个RBM网络的输入层都是其对应的真实数据，且将值归一化到了（0,1）。而其它RBM的输入层都是上一个RBM网络输出层的概率值；但是在实际的网络结构中，除了最底层的输入层和最顶层RBM的隐含层是连续值外，其它所有层都是一个二值随机变量。 此时最顶层RBM的隐含层是一个高斯分布的随机变量，其均值由该RBM的输入值决定，方差为1。

## 2. matlab代码复现
Hilton主页
http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html


code主要是2个单独的工程。

- 用MNIST数据库来进行深度的autoencoder压缩，用的是无监督学习，评价标准是重构误差值MSE。

- MNIST的手写字体识别，网络的预训练部分用的是无监督的，网络的微调部分用的是有监督的。评价标准准是识别率或者错误率。
